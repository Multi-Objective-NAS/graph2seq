{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nasbench'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bc380ee364ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnasbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_metrics_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nasbench'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from nasbench.lib import config\n",
    "from nasbench.lib import evaluate\n",
    "from nasbench.lib import model_metrics_pb2\n",
    "from nasbench.lib import model_spec as _model_spec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Bring ModelSpec to top-level for convenience. See lib/model_spec.py.\n",
    "ModelSpec = _model_spec.ModelSpec\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-194aa37acfc3>, line 87)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-194aa37acfc3>\"\u001b[0;36m, line \u001b[0;32m87\u001b[0m\n\u001b[0;31m    numpy as np\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\"User interface for the NAS Benchmark dataset.\n",
    "\n",
    "Before using this API, download the data files from the links in the README.\n",
    "\n",
    "Usage:\n",
    "  # Load the data from file (this will take some time)\n",
    "  nasbench = api.NASBench('/path/to/nasbench.tfrecord')\n",
    "\n",
    "  # Create an Inception-like module (5x5 convolution replaced with two 3x3\n",
    "  # convolutions).\n",
    "  model_spec = api.ModelSpec(\n",
    "      # Adjacency matrix of the module\n",
    "      matrix=[[0, 1, 1, 1, 0, 1, 0],    # input layer\n",
    "              [0, 0, 0, 0, 0, 0, 1],    # 1x1 conv\n",
    "              [0, 0, 0, 0, 0, 0, 1],    # 3x3 conv\n",
    "              [0, 0, 0, 0, 1, 0, 0],    # 5x5 conv (replaced by two 3x3's)\n",
    "              [0, 0, 0, 0, 0, 0, 1],    # 5x5 conv (replaced by two 3x3's)\n",
    "              [0, 0, 0, 0, 0, 0, 1],    # 3x3 max-pool\n",
    "              [0, 0, 0, 0, 0, 0, 0]],   # output layer\n",
    "      # Operations at the vertices of the module, matches order of matrix\n",
    "      ops=[INPUT, CONV1X1, CONV3X3, CONV3X3, CONV3X3, MAXPOOL3X3, OUTPUT])\n",
    "\n",
    "\n",
    "  # Query this model from dataset\n",
    "  data = nasbench.query(model_spec)\n",
    "\n",
    "Adjacency matrices are expected to be upper-triangular 0-1 matrices within the\n",
    "defined search space (7 vertices, 9 edges, 3 allowed ops). The first and last\n",
    "operations must be 'input' and 'output'. The other operations should be from\n",
    "config['available_ops']. Currently, the available operations are:\n",
    "  CONV3X3 = \"conv3x3-bn-relu\"\n",
    "  CONV1X1 = \"conv1x1-bn-relu\"\n",
    "  MAXPOOL3X3 = \"maxpool3x3\"\n",
    "\n",
    "When querying a spec, the spec will first be automatically pruned (removing\n",
    "unused vertices and edges along with ops). If the pruned spec is still out of\n",
    "the search space, an OutOfDomainError will be raised, otherwise the data is\n",
    "returned.\n",
    "\n",
    "The returned data object is a dictionary with the following keys:\n",
    "  - module_adjacency: numpy array for the adjacency matrix\n",
    "  - module_operations: list of operation labels\n",
    "  - trainable_parameters: number of trainable parameters in the model\n",
    "  - training_time: the total training time in seconds up to this point\n",
    "  - train_accuracy: training accuracy\n",
    "  - validation_accuracy: validation_accuracy\n",
    "  - test_accuracy: testing accuracy\n",
    "\n",
    "Instead of querying the dataset for a single run of a model, it is also possible\n",
    "to retrieve all metrics for a given spec, using:\n",
    "\n",
    "  fixed_stats, computed_stats = nasbench.get_metrics_from_spec(model_spec)\n",
    "\n",
    "The fixed_stats is a dictionary with the keys:\n",
    "  - module_adjacency\n",
    "  - module_operations\n",
    "  - trainable_parameters\n",
    "\n",
    "The computed_stats is a dictionary from epoch count to a list of metric\n",
    "dicts. For example, computed_stats[108][0] contains the metrics for the first\n",
    "repeat of the provided model trained to 108 epochs. The available keys are:\n",
    "  - halfway_training_time\n",
    "  - halfway_train_accuracy\n",
    "  - halfway_validation_accuracy\n",
    "  - halfway_test_accuracy\n",
    "  - final_training_time\n",
    "  - final_train_accuracy\n",
    "  - final_validation_accuracy\n",
    "  - final_test_accuracy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from nasbench.lib import config\n",
    "from nasbench.lib import evaluate\n",
    "from nasbench.lib import model_metrics_pb2\n",
    "from nasbench.lib import model_spec as _model_spec\n",
    " numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Bring ModelSpec to top-level for convenience. See lib/model_spec.py.\n",
    "ModelSpec = _model_spec.ModelSpec\n",
    "\n",
    "\n",
    "class OutOfDomainError(Exception):\n",
    "  \"\"\"Indicates that the requested graph is outside of the search domain.\"\"\"\n",
    "\n",
    "\n",
    "class NASBench(object):\n",
    "  \"\"\"User-facing API for accessing the NASBench dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, dataset_file, seed=None):\n",
    "    \"\"\"Initialize dataset, this should only be done once per experiment.\n",
    "\n",
    "    Args:\n",
    "      dataset_file: path to .tfrecord file containing the dataset.\n",
    "      seed: random seed used for sampling queried models. Two NASBench objects\n",
    "        created with the same seed will return the same data points when queried\n",
    "        with the same models in the same order. By default, the seed is randomly\n",
    "        generated.\n",
    "    \"\"\"\n",
    "    self.config = config.build_config()\n",
    "    random.seed(seed)\n",
    "\n",
    "    print('Loading dataset from file... This may take a few minutes...')\n",
    "    start = time.time()\n",
    "\n",
    "    # Stores the fixed statistics that are independent of evaluation (i.e.,\n",
    "    # adjacency matrix, operations, and number of parameters).\n",
    "    # hash --> metric name --> scalar\n",
    "    self.fixed_statistics = {}\n",
    "\n",
    "    # Stores the statistics that are computed via training and evaluating the\n",
    "    # model on CIFAR-10. Statistics are computed for multiple repeats of each\n",
    "    # model at each max epoch length.\n",
    "    # hash --> epochs --> repeat index --> metric name --> scalar\n",
    "    self.computed_statistics = {}\n",
    "\n",
    "    # Valid queriable epoch lengths. {4, 12, 36, 108} for the full dataset or\n",
    "    # {108} for the smaller dataset with only the 108 epochs.\n",
    "    self.valid_epochs = set()\n",
    "\n",
    "    for serialized_row in tf.python_io.tf_record_iterator(dataset_file):\n",
    "      # Parse the data from the data file.\n",
    "      module_hash, epochs, raw_adjacency, raw_operations, raw_metrics = (\n",
    "          json.loads(serialized_row.decode('utf-8')))\n",
    "\n",
    "      dim = int(np.sqrt(len(raw_adjacency)))\n",
    "      adjacency = np.array([int(e) for e in list(raw_adjacency)], dtype=np.int8)\n",
    "      adjacency = np.reshape(adjacency, (dim, dim))\n",
    "      operations = raw_operations.split(',')\n",
    "      metrics = model_metrics_pb2.ModelMetrics.FromString(\n",
    "          base64.b64decode(raw_metrics))\n",
    "      print(\"adjacency\")\n",
    "      print(adjacency)\n",
    "\n",
    "      if module_hash not in self.fixed_statistics:\n",
    "        # First time seeing this module, initialize fixed statistics.\n",
    "        new_entry = {}\n",
    "        new_entry['module_adjacency'] = adjacency\n",
    "        new_entry['module_operations'] = operations\n",
    "        new_entry['trainable_parameters'] = metrics.trainable_parameters\n",
    "        self.fixed_statistics[module_hash] = new_entry\n",
    "        self.computed_statistics[module_hash] = {}\n",
    "\n",
    "      self.valid_epochs.add(epochs)\n",
    "\n",
    "      if epochs not in self.computed_statistics[module_hash]:\n",
    "        self.computed_statistics[module_hash][epochs] = []\n",
    "\n",
    "      # Each data_point consists of the metrics recorded from a single\n",
    "      # train-and-evaluation of a model at a specific epoch length.\n",
    "      data_point = {}\n",
    "\n",
    "      # Note: metrics.evaluation_data[0] contains the computed metrics at the\n",
    "      # start of training (step 0) but this is unused by this API.\n",
    "\n",
    "      # Evaluation statistics at the half-way point of training\n",
    "      half_evaluation = metrics.evaluation_data[1]\n",
    "      data_point['halfway_training_time'] = half_evaluation.training_time\n",
    "      data_point['halfway_train_accuracy'] = half_evaluation.train_accuracy\n",
    "      data_point['halfway_validation_accuracy'] = (\n",
    "          half_evaluation.validation_accuracy)\n",
    "      data_point['halfway_test_accuracy'] = half_evaluation.test_accuracy\n",
    "\n",
    "      # Evaluation statistics at the end of training\n",
    "      final_evaluation = metrics.evaluation_data[2]\n",
    "      data_point['final_training_time'] = final_evaluation.training_time\n",
    "      data_point['final_train_accuracy'] = final_evaluation.train_accuracy\n",
    "      data_point['final_validation_accuracy'] = (\n",
    "          final_evaluation.validation_accuracy)\n",
    "      data_point['final_test_accuracy'] = final_evaluation.test_accuracy\n",
    "\n",
    "      self.computed_statistics[module_hash][epochs].append(data_point)\n",
    "      print(data_point)\n",
    "      f = open(\"datapoint.txt\", \"w\")\n",
    "      \n",
    "      \n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print('Loaded dataset in %d seconds' % elapsed)\n",
    "\n",
    "    self.history = {}\n",
    "    self.training_time_spent = 0.0\n",
    "    self.total_epochs_spent = 0\n",
    "\n",
    "  def query(self, model_spec, epochs=108, stop_halfway=False):\n",
    "    \"\"\"Fetch one of the evaluations for this model spec.\n",
    "\n",
    "    Each call will sample one of the config['num_repeats'] evaluations of the\n",
    "    model. This means that repeated queries of the same model (or isomorphic\n",
    "    models) may return identical metrics.\n",
    "\n",
    "    This function will increment the budget counters for benchmarking purposes.\n",
    "    See self.training_time_spent, and self.total_epochs_spent.\n",
    "\n",
    "    This function also allows querying the evaluation metrics at the halfway\n",
    "    point of training using stop_halfway. Using this option will increment the\n",
    "    budget counters only up to the halfway point.\n",
    "\n",
    "    Args:\n",
    "      model_spec: ModelSpec object.\n",
    "      epochs: number of epochs trained. Must be one of the evaluated number of\n",
    "        epochs, [4, 12, 36, 108] for the full dataset.\n",
    "      stop_halfway: if True, returned dict will only contain the training time\n",
    "        and accuracies at the halfway point of training (num_epochs/2).\n",
    "        Otherwise, returns the time and accuracies at the end of training\n",
    "        (num_epochs).\n",
    "\n",
    "    Returns:\n",
    "      dict containing the evaluated data for this object.\n",
    "\n",
    "    Raises:\n",
    "      OutOfDomainError: if model_spec or num_epochs is outside the search space.\n",
    "    \"\"\"\n",
    "    if epochs not in self.valid_epochs:\n",
    "      raise OutOfDomainError('invalid number of epochs, must be one of %s'\n",
    "                             % self.valid_epochs)\n",
    "\n",
    "    fixed_stat, computed_stat = self.get_metrics_from_spec(model_spec)\n",
    "    sampled_index = random.randint(0, self.config['num_repeats'] - 1)\n",
    "    computed_stat = computed_stat[epochs][sampled_index]\n",
    "\n",
    "    data = {}\n",
    "    data['module_adjacency'] = fixed_stat['module_adjacency']\n",
    "    data['module_operations'] = fixed_stat['module_operations']\n",
    "    data['trainable_parameters'] = fixed_stat['trainable_parameters']\n",
    "\n",
    "    if stop_halfway:\n",
    "      data['training_time'] = computed_stat['halfway_training_time']\n",
    "      data['train_accuracy'] = computed_stat['halfway_train_accuracy']\n",
    "      data['validation_accuracy'] = computed_stat['halfway_validation_accuracy']\n",
    "      data['test_accuracy'] = computed_stat['halfway_test_accuracy']\n",
    "    else:\n",
    "      data['training_time'] = computed_stat['final_training_time']\n",
    "      data['train_accuracy'] = computed_stat['final_train_accuracy']\n",
    "      data['validation_accuracy'] = computed_stat['final_validation_accuracy']\n",
    "      data['test_accuracy'] = computed_stat['final_test_accuracy']\n",
    "\n",
    "    self.training_time_spent += data['training_time']\n",
    "    if stop_halfway:\n",
    "      self.total_epochs_spent += epochs // 2\n",
    "    else:\n",
    "      self.total_epochs_spent += epochs\n",
    "\n",
    "    return data\n",
    "\n",
    "  def is_valid(self, model_spec):\n",
    "    \"\"\"Checks the validity of the model_spec.\n",
    "\n",
    "    For the purposes of benchmarking, this does not increment the budget\n",
    "    counters.\n",
    "\n",
    "    Args:\n",
    "      model_spec: ModelSpec object.\n",
    "\n",
    "    Returns:\n",
    "      True if model is within space.\n",
    "    \"\"\"\n",
    "    try:\n",
    "      self._check_spec(model_spec)\n",
    "    except OutOfDomainError:\n",
    "      return False\n",
    "\n",
    "    return True\n",
    "\n",
    "  def get_budget_counters(self):\n",
    "    \"\"\"Returns the time and budget counters.\"\"\"\n",
    "    return self.training_time_spent, self.total_epochs_spent\n",
    "\n",
    "  def reset_budget_counters(self):\n",
    "    \"\"\"Reset the time and epoch budget counters.\"\"\"\n",
    "    self.training_time_spent = 0.0\n",
    "    self.total_epochs_spent = 0\n",
    "\n",
    "  def evaluate(self, model_spec, model_dir):\n",
    "    \"\"\"Trains and evaluates a model spec from scratch (does not query dataset).\n",
    "\n",
    "    This function runs the same procedure that was used to generate each\n",
    "    evaluation in the dataset.  Because we are not querying the generated\n",
    "    dataset of trained models, there are no limitations on number of vertices,\n",
    "    edges, operations, or epochs. Note that the results will not exactly match\n",
    "    the dataset due to randomness. By default, this uses TPUs for evaluation but\n",
    "    CPU/GPU can be used by setting --use_tpu=false (GPU will require installing\n",
    "    tensorflow-gpu).\n",
    "\n",
    "    Args:\n",
    "      model_spec: ModelSpec object.\n",
    "      model_dir: directory to store the checkpoints, summaries, and logs.\n",
    "\n",
    "    Returns:\n",
    "      dict contained the evaluated data for this object, same structure as\n",
    "      returned by query().\n",
    "    \"\"\"\n",
    "    # Metadata contains additional metrics that aren't reported normally.\n",
    "    # However, these are stored in the JSON file at the model_dir.\n",
    "    metadata = evaluate.train_and_evaluate(model_spec, self.config, model_dir)\n",
    "    metadata_file = os.path.join(model_dir, 'metadata.json')\n",
    "    with tf.gfile.Open(metadata_file, 'w') as f:\n",
    "      json.dump(metadata, f, cls=_NumpyEncoder)\n",
    "\n",
    "    data_point = {}\n",
    "    data_point['module_adjacency'] = model_spec.matrix\n",
    "    data_point['module_operations'] = model_spec.ops\n",
    "    data_point['trainable_parameters'] = metadata['trainable_params']\n",
    "\n",
    "    final_evaluation = metadata['evaluation_results'][-1]\n",
    "    data_point['training_time'] = final_evaluation['training_time']\n",
    "    data_point['train_accuracy'] = final_evaluation['train_accuracy']\n",
    "    data_point['validation_accuracy'] = final_evaluation['validation_accuracy']\n",
    "    data_point['test_accuracy'] = final_evaluation['test_accuracy']\n",
    "\n",
    "    return data_point\n",
    "\n",
    "  def hash_iterator(self):\n",
    "    \"\"\"Returns iterator over all unique model hashes.\"\"\"\n",
    "    return self.fixed_statistics.keys()\n",
    "\n",
    "  def get_metrics_from_hash(self, module_hash):\n",
    "    \"\"\"Returns the metrics for all epochs and all repeats of a hash.\n",
    "\n",
    "    This method is for dataset analysis and should not be used for benchmarking.\n",
    "    As such, it does not increment any of the budget counters.\n",
    "\n",
    "    Args:\n",
    "      module_hash: MD5 hash, i.e., the values yielded by hash_iterator().\n",
    "\n",
    "    Returns:\n",
    "      fixed stats and computed stats of the model spec provided.\n",
    "    \"\"\"\n",
    "    fixed_stat = copy.deepcopy(self.fixed_statistics[module_hash])\n",
    "    computed_stat = copy.deepcopy(self.computed_statistics[module_hash])\n",
    "    return fixed_stat, computed_stat\n",
    "\n",
    "  def get_metrics_from_spec(self, model_spec):\n",
    "    \"\"\"Returns the metrics for all epochs and all repeats of a model.\n",
    "\n",
    "    This method is for dataset analysis and should not be used for benchmarking.\n",
    "    As such, it does not increment any of the budget counters.\n",
    "\n",
    "    Args:\n",
    "      model_spec: ModelSpec object.\n",
    "\n",
    "    Returns:\n",
    "      fixed stats and computed stats of the model spec provided.\n",
    "    \"\"\"\n",
    "    self._check_spec(model_spec)\n",
    "    module_hash = self._hash_spec(model_spec)\n",
    "    return self.get_metrics_from_hash(module_hash)\n",
    "\n",
    "  def _check_spec(self, model_spec):\n",
    "    \"\"\"Checks that the model spec is within the dataset.\"\"\"\n",
    "    if not model_spec.valid_spec:\n",
    "      raise OutOfDomainError('invalid spec, provided graph is disconnected.')\n",
    "\n",
    "    num_vertices = len(model_spec.ops)\n",
    "    num_edges = np.sum(model_spec.matrix)\n",
    "\n",
    "    if num_vertices > self.config['module_vertices']:\n",
    "      raise OutOfDomainError('too many vertices, got %d (max vertices = %d)'\n",
    "                             % (num_vertices, config['module_vertices']))\n",
    "\n",
    "    if num_edges > self.config['max_edges']:\n",
    "      raise OutOfDomainError('too many edges, got %d (max edges = %d)'\n",
    "                             % (num_edges, self.config['max_edges']))\n",
    "\n",
    "    if model_spec.ops[0] != 'input':\n",
    "      raise OutOfDomainError('first operation should be \\'input\\'')\n",
    "    if model_spec.ops[-1] != 'output':\n",
    "      raise OutOfDomainError('last operation should be \\'output\\'')\n",
    "    for op in model_spec.ops[1:-1]:\n",
    "      if op not in self.config['available_ops']:\n",
    "        raise OutOfDomainError('unsupported op %s (available ops = %s)'\n",
    "                               % (op, self.config['available_ops']))\n",
    "\n",
    "  def _hash_spec(self, model_spec):\n",
    "    \"\"\"Returns the MD5 hash for a provided model_spec.\"\"\"\n",
    "    return model_spec.hash_spec(self.config['available_ops'])\n",
    "\n",
    "\n",
    "class _NumpyEncoder(json.JSONEncoder):\n",
    "  \"\"\"Converts numpy objects to JSON-serializable format.\"\"\"\n",
    "\n",
    "  def default(self, obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "      # Matrices converted to nested lists\n",
    "      return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "      # Scalars converted to closest Python type\n",
    "      return np.asscalar(obj)\n",
    "    return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-6-9591fcb426da>, line 33)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-9591fcb426da>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    module_hash, epochs, raw_adjacency, raw_operations, raw_metrics = (\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Initialize dataset, this should only be done once per experiment.\n",
    "\n",
    "Args:\n",
    "  dataset_file: path to .tfrecord file containing the dataset.\n",
    "  seed: random seed used for sampling queried models. Two NASBench objects\n",
    "    created with the same seed will return the same data points when queried\n",
    "    with the same models in the same order. By default, the seed is randomly\n",
    "    generated.\n",
    "\"\"\"\n",
    "config = config.build_config()\n",
    "random.seed(seed)\n",
    "\n",
    "print('Loading dataset from file... This may take a few minutes...')\n",
    "start = time.time()\n",
    "\n",
    "# Stores the fixed statistics that are independent of evaluation (i.e.,\n",
    "# adjacency matrix, operations, and number of parameters).\n",
    "# hash --> metric name --> scalar\n",
    "fixed_statistics = {}\n",
    "\n",
    "# Stores the statistics that are computed via training and evaluating the\n",
    "# model on CIFAR-10. Statistics are computed for multiple repeats of each\n",
    "# model at each max epoch length.\n",
    "# hash --> epochs --> repeat index --> metric name --> scalar\n",
    "computed_statistics = {}\n",
    "\n",
    "# Valid queriable epoch lengths. {4, 12, 36, 108} for the full dataset or\n",
    "# {108} for the smaller dataset with only the 108 epochs.\n",
    "valid_epochs = set()\n",
    "\n",
    "\n",
    "  # Parse the data from the data file.\n",
    "  module_hash, epochs, raw_adjacency, raw_operations, raw_metrics = (\n",
    "      json.loads(serialized_row.decode('utf-8')))\n",
    "\n",
    "  dim = int(np.sqrt(len(raw_adjacency)))\n",
    "  adjacency = np.array([int(e) for e in list(raw_adjacency)], dtype=np.int8)\n",
    "  adjacency = np.reshape(adjacency, (dim, dim))\n",
    "  operations = raw_operations.split(',')\n",
    "  metrics = model_metrics_pb2.ModelMetrics.FromString(\n",
    "      base64.b64decode(raw_metrics))\n",
    "  print(\"adjacency\")\n",
    "  print(adjacency)\n",
    "\n",
    "  if module_hash not in fixed_statistics:\n",
    "    # First time seeing this module, initialize fixed statistics.\n",
    "    new_entry = {}\n",
    "    new_entry['module_adjacency'] = adjacency\n",
    "    new_entry['module_operations'] = operations\n",
    "    new_entry['trainable_parameters'] = metrics.trainable_parameters\n",
    "    fixed_statistics[module_hash] = new_entry\n",
    "    computed_statistics[module_hash] = {}\n",
    "\n",
    "  valid_epochs.add(epochs)\n",
    "\n",
    "  if epochs not in computed_statistics[module_hash]:\n",
    "    computed_statistics[module_hash][epochs] = []\n",
    "\n",
    "  # Each data_point consists of the metrics recorded from a single\n",
    "  # train-and-evaluation of a model at a specific epoch length.\n",
    "  data_point = {}\n",
    "\n",
    "  # Note: metrics.evaluation_data[0] contains the computed metrics at the\n",
    "  # start of training (step 0) but this is unused by this API.\n",
    "\n",
    "  # Evaluation statistics at the half-way point of training\n",
    "  half_evaluation = metrics.evaluation_data[1]\n",
    "  data_point['halfway_training_time'] = half_evaluation.training_time\n",
    "  data_point['halfway_train_accuracy'] = half_evaluation.train_accuracy\n",
    "  data_point['halfway_validation_accuracy'] = (\n",
    "      half_evaluation.validation_accuracy)\n",
    "  data_point['halfway_test_accuracy'] = half_evaluation.test_accuracy\n",
    "\n",
    "  # Evaluation statistics at the end of training\n",
    "  final_evaluation = metrics.evaluation_data[2]\n",
    "  data_point['final_training_time'] = final_evaluation.training_time\n",
    "  data_point['final_train_accuracy'] = final_evaluation.train_accuracy\n",
    "  data_point['final_validation_accuracy'] = (\n",
    "      final_evaluation.validation_accuracy)\n",
    "  data_point['final_test_accuracy'] = final_evaluation.test_accuracy\n",
    "\n",
    "  computed_statistics[module_hash][epochs].append(data_point)\n",
    "  print(data_point)\n",
    "  f = open(\"datapoint.txt\", \"w\")\n",
    "  \n",
    "  \n",
    "\n",
    "elapsed = time.time() - start\n",
    "print('Loaded dataset in %d seconds' % elapsed)\n",
    "\n",
    "history = {}\n",
    "training_time_spent = 0.0\n",
    "total_epochs_spent = 0\n",
    "\n",
    "def query( model_spec, epochs=108, stop_halfway=False):\n",
    "\"\"\"Fetch one of the evaluations for this model spec.\n",
    "\n",
    "Each call will sample one of the config['num_repeats'] evaluations of the\n",
    "model. This means that repeated queries of the same model (or isomorphic\n",
    "models) may return identical metrics.\n",
    "\n",
    "This function will increment the budget counters for benchmarking purposes.\n",
    "See training_time_spent, and total_epochs_spent.\n",
    "\n",
    "This function also allows querying the evaluation metrics at the halfway\n",
    "point of training using stop_halfway. Using this option will increment the\n",
    "budget counters only up to the halfway point.\n",
    "\n",
    "Args:\n",
    "  model_spec: ModelSpec object.\n",
    "  epochs: number of epochs trained. Must be one of the evaluated number of\n",
    "    epochs, [4, 12, 36, 108] for the full dataset.\n",
    "  stop_halfway: if True, returned dict will only contain the training time\n",
    "    and accuracies at the halfway point of training (num_epochs/2).\n",
    "    Otherwise, returns the time and accuracies at the end of training\n",
    "    (num_epochs).\n",
    "\n",
    "Returns:\n",
    "  dict containing the evaluated data for this object.\n",
    "\n",
    "Raises:\n",
    "  OutOfDomainError: if model_spec or num_epochs is outside the search space.\n",
    "\"\"\"\n",
    "if epochs not in valid_epochs:\n",
    "  raise OutOfDomainError('invalid number of epochs, must be one of %s'\n",
    "                         % valid_epochs)\n",
    "\n",
    "fixed_stat, computed_stat = get_metrics_from_spec(model_spec)\n",
    "sampled_index = random.randint(0, config['num_repeats'] - 1)\n",
    "computed_stat = computed_stat[epochs][sampled_index]\n",
    "\n",
    "data = {}\n",
    "data['module_adjacency'] = fixed_stat['module_adjacency']\n",
    "data['module_operations'] = fixed_stat['module_operations']\n",
    "data['trainable_parameters'] = fixed_stat['trainable_parameters']\n",
    "\n",
    "if stop_halfway:\n",
    "  data['training_time'] = computed_stat['halfway_training_time']\n",
    "  data['train_accuracy'] = computed_stat['halfway_train_accuracy']\n",
    "  data['validation_accuracy'] = computed_stat['halfway_validation_accuracy']\n",
    "  data['test_accuracy'] = computed_stat['halfway_test_accuracy']\n",
    "else:\n",
    "  data['training_time'] = computed_stat['final_training_time']\n",
    "  data['train_accuracy'] = computed_stat['final_train_accuracy']\n",
    "  data['validation_accuracy'] = computed_stat['final_validation_accuracy']\n",
    "  data['test_accuracy'] = computed_stat['final_test_accuracy']\n",
    "\n",
    "training_time_spent += data['training_time']\n",
    "if stop_halfway:\n",
    "  total_epochs_spent += epochs // 2\n",
    "else:\n",
    "  total_epochs_spent += epochs\n",
    "\n",
    "return data"
   ]
  }
 ]
}