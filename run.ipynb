{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import model_utils.configure as conf\n",
    "\"\"\"\n",
    "0: sos/eos\n",
    "1: no connection\n",
    "2: connection\n",
    "3: CONV1X1\n",
    "4: CONV3X3\n",
    "5: MAXPOOL3X3\n",
    "6: OUTPUT\n",
    "\"\"\"\n",
    "        \n",
    "class ControllerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        super(ControllerDataset, self).__init__()\n",
    "\n",
    "        self.adjacency_matrices = []\n",
    "        self.operations = []\n",
    "        self.sequences = []\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "\n",
    "                jo = json.loads(line, object_pairs_hook=OrderedDict)\n",
    "\n",
    "                self.adjacency_matrices.append(jo['module_adjacency'])\n",
    "                self.operations.append(jo['module_operations'])\n",
    "                self.sequences.append(jo['sequence'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        operations = self.operations[index]\n",
    "        num_nodes = len(operations)\n",
    "\n",
    "        ops = []\n",
    "        for op in operations:\n",
    "            if op == CONV1X1:\n",
    "                ops.append(3)\n",
    "            elif op == CONV3X3:\n",
    "                ops.append(4)\n",
    "            elif op == MAXPOOL3X3:\n",
    "                ops.append(5)\n",
    "            elif op == OUTPUT:\n",
    "                ops.append(6)\n",
    "            if op == INPUT:\n",
    "                ops.append(7)\n",
    "\n",
    "        sample = {\n",
    "            'matrix' : torch.LongTensor(self.adjacency_matrices[index]),\n",
    "            'operations': torch.LongTensor(ops),\n",
    "            'sequence': torch.LongTensor(self.sequences[index])\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    ## transform into batch samples\n",
    "\n",
    "    ## create node -> global idnex\n",
    "    ## global index -> neighbor's global index\n",
    "\n",
    "    degree_max_size = conf.degree_max_size\n",
    "    graph_size = conf.graph_size\n",
    "    # degree_max_size = 5\n",
    "    # graph_size = 7\n",
    "    seq_max_length = int((graph_size+2)*(graph_size-1)/2)\n",
    "\n",
    "    g_idxs = []\n",
    "    g_fw_adjs = []\n",
    "    g_bw_adjs = []\n",
    "    g_operations = []\n",
    "    g_sequence = []\n",
    "    g_num_nodes = []\n",
    "\n",
    "    g_idx_base = 0\n",
    "    for g_idx, sample in enumerate(samples):\n",
    "        matrix = sample['matrix']\n",
    "        num_nodes = matrix.shape[0]\n",
    "        g_num_nodes.append(num_nodes)\n",
    "\n",
    "        for row in range(num_nodes):\n",
    "            g_fw_adjs.append(list())\n",
    "            g_bw_adjs.append(list())\n",
    "\n",
    "        for row in range(num_nodes):\n",
    "           for col in range(row+1, num_nodes):\n",
    "            if matrix[row][col] :\n",
    "                g_fw_adjs[g_idx_base + row].append(g_idx_base + col)\n",
    "                g_bw_adjs[g_idx_base + col].append(g_idx_base + row)\n",
    "\n",
    "        for op in sample['operations']:\n",
    "            g_operations.append(op)\n",
    "\n",
    "        sequence = sample['sequence']\n",
    "\n",
    "        sequence = torch.cat([sequence, torch.LongTensor([0] * (seq_max_length - len(sequence)))])\n",
    "        g_sequence.append(sequence)\n",
    "\n",
    "        g_idx_base += num_nodes\n",
    "\n",
    "    for idx in range(len(g_fw_adjs)):\n",
    "        g_fw_adjs[idx].extend([g_idx_base] * (degree_max_size - len(g_fw_adjs[idx])))\n",
    "        g_bw_adjs[idx].extend([g_idx_base] * (degree_max_size - len(g_bw_adjs[idx])))\n",
    "        \n",
    "    g_operations.append(0)\n",
    "\n",
    "    g_num_nodes = torch.LongTensor(g_num_nodes)\n",
    "\n",
    "    # [batch_size, conf.degree_max_size]\n",
    "    g_fw_adjs = torch.LongTensor(g_fw_adjs)\n",
    "    g_bw_adjs = torch.LongTensor(g_bw_adjs)\n",
    "\n",
    "    # [batch_size +1] # due to padding\n",
    "    g_operations = torch.LongTensor(g_operations)\n",
    "\n",
    "    # [sum of sequence_length]\n",
    "    g_sequence = torch.stack(g_sequence)\n",
    "    print(\"g_fw_adjs size: {}\".format(g_fw_adjs.size()))\n",
    "    print(\"g_bw_adjs.size() : {}\".format(g_bw_adjs.size()))\n",
    "    print(\"num_nodes.size() : {}\".format(num_nodes.size()))\n",
    "    print(\"operation.size() : {}\".format(operation.size()))\n",
    "    print(\"operation.size() : {}\".format(operation.size()))\n",
    "\n",
    "    return {\n",
    "            'num_nodes' : g_num_nodes,\n",
    "            'fw_adjs': g_fw_adjs,\n",
    "            'bw_adjs': g_bw_adjs,\n",
    "            'operations': g_operations,\n",
    "            'sequence': g_sequence\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from layers import Layer, Dense\n",
    "from inits import glorot, zeros\n",
    "\"\"\"\n",
    "\n",
    "class tMeanAggregator(nn.Module):\n",
    "    \"\"\"Aggregates via mean followed by matmul and non-linearity.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n",
    "            dropout=0, withBias=True, act=F.relu,\n",
    "            concat=False, mode=\"train\", **kwargs):\n",
    "        super(tMeanAggregator, self).__init__(**kwargs)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.withBias = withBias\n",
    "        self.act = act\n",
    "        self.concat = concat\n",
    "        self.mode = mode\n",
    "\n",
    "        \n",
    "        if neigh_input_dim == None:\n",
    "            neigh_input_dim = input_dim\n",
    "\n",
    "        if concat:\n",
    "            self.output_dim = 2 * output_dim\n",
    "\n",
    "        self.neigh_weights = nn.init.xavier_uniform_(torch.empty(neigh_input_dim, output_dim))\n",
    "        self.self_weights = nn.init.xavier_uniform_(torch.empty(input_dim, output_dim))\n",
    "        \n",
    "        if self.withBias:\n",
    "            self.bias = torch.zeros(self.output_dim)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, self_vecs, neigh_vecs, neigh_len=0):\n",
    "        if self.mode == \"train\":\n",
    "            neigh_vecs = F.dropout(neigh_vecs, self.dropout)\n",
    "            self_vecs = F.dropout(self_vecs, self.dropout)\n",
    "\n",
    "        # reduce_mean performs better than mean_pool\n",
    "        neigh_means = torch.mean(neigh_vecs, dim=1)\n",
    "        # neigh_means = mean_pool(neigh_vecs, neigh_len)\n",
    "\n",
    "        # [nodes] x [out_dim]\n",
    "        from_neighs = torch.matmul(neigh_means, self.neigh_weights)\n",
    "        from_self = torch.matmul(self_vecs, self.self_weights)\n",
    "\n",
    "        if not self.concat:\n",
    "            output = torch.add(from_self, from_neighs)\n",
    "        else:\n",
    "            output = torch.cat([from_self, from_neighs], dim=1)\n",
    "\n",
    "        # bias\n",
    "        if self.withBias:\n",
    "            output += self.bias\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import model_utils.configure as conf\n",
    "from model import Graph2Seq\n",
    "import utils\n",
    "\n",
    "import math\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "INPUT = 'input'\n",
    "CONV1X1 = 'conv1x1-bn-relu'\n",
    "CONV3X3 = 'conv3x3-bn-relu'\n",
    "MAXPOOL3X3 = 'maxpool3x3'\n",
    "OUTPUT = 'output'\n",
    "\n",
    "mode = \"train\"\n",
    "random.seed(conf.seed)\n",
    "np.random.seed(conf.seed)\n",
    "torch.manual_seed(conf.seed)\n",
    "logging.info(\"conf = %s\", conf)\n",
    "\n",
    "conf.source_length = conf.encoder_length = conf.decoder_length = (conf.nodes + 2) * (conf.nodes - 1) // 2\n",
    "epochs = conf.epochs\n",
    "\n",
    "\n",
    "#model = Graph2Seq(mode=mode, conf=conf)\n",
    "\n",
    "# load data\n",
    "dataset = utils.ControllerDataset(conf.data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SOS_ID = 0\n",
    "EOS_ID = 0\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, source_dim=None, output_dim=None, bias=False):\n",
    "        super(Attention, self).__init__()\n",
    "        if source_dim is None:\n",
    "            source_dim = input_dim\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.source_dim = source_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.input_proj = nn.Linear(input_dim, source_dim, bias=bias)\n",
    "        self.output_proj = nn.Linear(input_dim + source_dim, output_dim, bias=bias)\n",
    "    \n",
    "    def forward(self, input, source_hids, mask=None):\n",
    "        batch_size = input.size(0)\n",
    "        source_len = source_hids.size(1)\n",
    "\n",
    "        # (batch, tgt_len, input_dim) -> (batch, tgt_len, source_dim)\n",
    "        x = self.input_proj(input)\n",
    "\n",
    "        # (batch, tgt_len, source_dim) * (batch, src_len, source_dim) -> (batch, tgt_len, src_len)\n",
    "        attn = torch.bmm(x, source_hids.transpose(1, 2))\n",
    "        if mask is not None:\n",
    "            attn.data.masked_fill_(mask, -float('inf'))\n",
    "        attn = F.softmax(attn.view(-1, source_len), dim=1).view(batch_size, -1, source_len)\n",
    "        \n",
    "        # (batch, tgt_len, src_len) * (batch, src_len, source_dim) -> (batch, tgt_len, source_dim)\n",
    "        mix = torch.bmm(attn, source_hids)\n",
    "        \n",
    "        # concat -> (batch, tgt_len, source_dim + input_dim)\n",
    "        combined = torch.cat((mix, input), dim=2)\n",
    "        # output -> (batch, tgt_len, output_dim)\n",
    "        output = torch.tanh(self.output_proj(combined.view(-1, self.input_dim + self.source_dim))).view(batch_size, -1, self.output_dim)\n",
    "        \n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                mode,\n",
    "                hidden_dim,\n",
    "                embedding_vocab_size,\n",
    "                decoder_vocab_size,\n",
    "                dropout,\n",
    "                length,\n",
    "                layers\n",
    "                ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.length = length\n",
    "        self.embedding_vocab_size = embedding_vocab_size\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.layers = layers\n",
    "        self.rnn = nn.LSTM(self.hidden_dim, self.hidden_dim, self.layers, batch_first=True, dropout=dropout)\n",
    "        self.init_input = None\n",
    "        self.embedding = nn.Embedding(self.embedding_vocab_size, self.hidden_dim)\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.decoder_vocab_size)\n",
    "        self.n = int(math.floor(math.sqrt((self.length + 1) * 2)))\n",
    "        self.offsets=[]\n",
    "        for i in range(self.n):\n",
    "            self.offsets.append( (i + 3) * i // 2 - 1)\n",
    "    \n",
    "    def forward(self, x, num_nodes, initial_states=None, encoder_hidden=None, targets=None):\n",
    "\n",
    "        \"\"\"\n",
    "        x는 initial input of each batch\n",
    "        that is, [batch_size, graph_embeddindg_dimension]\n",
    "        \"\"\"\n",
    "        ## train이든 test 이든\n",
    "        ## graph embedding을 첫 input으로 받아서 -> 계속해서 전 단계의 embedding을 받겟지.\n",
    "\n",
    "        ## encoder last state와 attention : 이때 graph embedding은 제외 -> decoder output\n",
    "\n",
    "        ## train이면 output과 실제 target을 비교하고, 다음 input은 target의 element 값이 되고\n",
    "        ## loss function을 비교해서 gradient update\n",
    "\n",
    "        ## test이면 output이 다음 input이 되고, loss function로 정확도만 계산. no gradient update\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            batch_size = x.size(0)\n",
    "            target_length = targets.size(1)\n",
    "            # targets to decoder input\n",
    "            x = torch.Tensor().new_full((batch_size, 1), 0, dtype=torch.long, requires_grad=True)\n",
    "            x = torch.cat([x, targets], dim=1)\n",
    "            print(\"after cat : {}\".format(x.size()))\n",
    "            x = self.embedding(x)\n",
    "            print(\"after embedding : {}\".format(x.size()))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "            residual = x\n",
    "            \"\"\"\n",
    "            h_0: shape (num_layers * num_directions, batch, hidden_size): initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1.\n",
    "            c_0: shape (num_layers * num_directions, batch, hidden_size): initial cell state for each element in the batch.\n",
    "            \"\"\"\n",
    "            print(\"initial states : {}\".format(initial_states[0].size()))\n",
    "            x, hidden = self.rnn(x, initial_states)\n",
    "            x = (residual + x) * math.sqrt(0.5)\n",
    "            residual = x\n",
    "            x, _ = self.attention(x, encoder_hidden)\n",
    "            x = (residual + x) * math.sqrt(0.5)\n",
    "\n",
    "            predicted_softmax = F.log_softmax(self.out(x.view(-1, self.hidden_dim)), dim=-1)\n",
    "            predicted_softmax = predicted_softmax.view(batch_size, self.decoder_vocab_size, -1)\n",
    "            print(\"predicted_softmax: {}\".format(predicted_softmax.size()))\n",
    "            predicted_softmax = predicted_softmax.view(batch_size, targets, -1)\n",
    "            # predicteed_softmax_list = torch.split(predicted_softmax, num_nodes.tolist())\n",
    "            # predicted_softmax = torch.nn.utils.rnn.pad_sequence(predicteed_softmax_list, batch_first=True, padding_value=0)\n",
    "        \n",
    "            return predicted_softmax, None\n",
    "\n",
    "        # x : list of graph embeddings\n",
    "        elif self.mode == \"test\":\n",
    "            batch_size = x.size(0)\n",
    "            length = max(num_nodes)\n",
    "            decoder_hidden = initial_states\n",
    "            \n",
    "            decoded_ids = torch.Tensor().new_full((batch_size, 1), 0, dtype=torch.long, requires_grad=False)\n",
    "            \n",
    "            def decode(step, output):\n",
    "                if step in self.offsets:  # sample operation, should be in [3, 7]\n",
    "                    if step != (self.n + 2) * (self.n - 1) / 2 - 1:\n",
    "                        symbol = output[:, 3:6].topk(1)[1] + 3\n",
    "                    else:\n",
    "                        symbol = output[:, 6:].topk(1)[1] + 6\n",
    "                else:  # sample connection, should be in [1, 2]\n",
    "                    symbol = output[:, 1:3].topk(1)[1] + 1\n",
    "                return symbol\n",
    "            \n",
    "            for i in range(length):\n",
    "                x = self.embedding(decoded_ids[:, i:i+1])\n",
    "                x = F.dropout(x, self.dropout, training=self.training)\n",
    "                residual = x\n",
    "                x, decoder_hidden = self.rnn(x, decoder_hidden)\n",
    "                x = (residual + x) * math.sqrt(0.5)\n",
    "                residual = x\n",
    "                x, _ = self.attention(x, encoder_hidden)\n",
    "                x = (residual + x) * math.sqrt(0.5)\n",
    "                output = self.out(x.squeeze(1))\n",
    "                symbol = decode(i, output)\n",
    "                decoded_ids = torch.cat((decoded_ids, symbol), axis=-1)\n",
    "                x = self.embedding(symbol)\n",
    "\n",
    "            return None, decoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "initial_states size : torch.Size([1, 2, 64])\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'decoder_input' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-147df0eaa5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# graph_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/g2s/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-13d58787e90f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, num_nodes, initial_states, encoder_hidden, targets)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_input' is not defined"
     ]
    }
   ],
   "source": [
    "initial_states = graph_embedding[0].unsqueeze(0)\n",
    "print(\"initial_states size : {}\".format(initial_states.size()))\n",
    "initial_states = tuple([initial_states, initial_states])\n",
    "\n",
    "de = Decoder(\n",
    "            mode=\"test\",\n",
    "            hidden_dim=conf.hidden_layer_dim * 4,\n",
    "            embedding_vocab_size=9,\n",
    "            decoder_vocab_size=7,\n",
    "            dropout=conf.dropout,\n",
    "            length=27,\n",
    "            layers=1\n",
    "            )\n",
    "\n",
    "# graph_embedding\n",
    "de(graph_embedding[0], num_nodes, initial_states=initial_states, encoder_hidden=encoder_hidden, targets=sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hop: 0, after aggregate: fw_hidden_size: torch.Size([14, 32])\nhop: 1, after aggregate: fw_hidden_size: torch.Size([14, 32])\nhop: 2, after aggregate: fw_hidden_size: torch.Size([14, 32])\nhop: 3, after aggregate: fw_hidden_size: torch.Size([14, 32])\nhop: 4, after aggregate: fw_hidden_size: torch.Size([14, 32])\nhop: 5, after aggregate: fw_hidden_size: torch.Size([14, 32])\n"
    }
   ],
   "source": [
    "queue = torch.utils.data.DataLoader(dataset, batch_size=conf.batch_size, shuffle=True, pin_memory=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "import model_utils.configure as conf\n",
    "from importlib import reload\n",
    "from encoder import Encoder\n",
    "\n",
    "encoder_hidden = []\n",
    "graph_embedding = []\n",
    "en = Encoder(\"train\", conf.vocab_size, conf.hidden_layer_dim, \"bi\", 6, True, conf.dropout, conf.learning_rate)\n",
    "\n",
    "i = 0\n",
    "for step, sample in enumerate(queue):\n",
    "    fw_adjs = sample['fw_adjs'] \n",
    "    bw_adjs = sample['bw_adjs'] \n",
    "    operations = sample['operations'] \n",
    "    num_nodes = sample['num_nodes'] \n",
    "    sequence = sample['sequence'] \n",
    "\n",
    "    encoder_hidden, graph_embedding = en(fw_adjs, bw_adjs, operations, num_nodes)\n",
    "    break\n",
    "    i+=1\n",
    "    if i == 1: break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5\n"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}